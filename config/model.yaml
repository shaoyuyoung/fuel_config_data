# This file includes the model configuration
local:
  model: "Qwen/Qwen2.5-Coder-32B-Instruct"  # HUGGINGFACE model path
  num: 1
  temperature: 1
  top_p: 1
  repetition_penalty: 1.05
  max_tokens: 4096
  dtype: "float16"  # @SHAOYU: using float32 because our GPU V100 not supports bfloat16, refer to https://github.com/vllm-project/vllm/issues/15235
  gpu_numbers: 4  # @SHAOYU: We have 4 V100 GPUs
  stop: [  # can be set to `String` or `List`. When meeting these words. API would stop to analyze more tokens.
    "<|endoftext|>",
    "###",
    "__output__ =",
    "if __name__",
    '"""',
    "'''",
    "# Model ends",
  ]
  swap_space: 20
  seed: "random.randint(0, 10000)"



server:
  model: "deepseek-chat" # @SHAOYU: model name is provided by your API server
  url: "https://api.deepseek.com/beta"  # @SHAOYU: similar with above
  key_file: "config/llm-key.txt"
  retry_times: 3
  frequency_penalty: 2  # range from [-2,2]. if set > 0, decreasing the model's likelihood to repeat the same line verbatim.
  max_tokens: 4096  # the max length of the output (Beta maximum 8192)
  presence_penalty: 2  # range from [-2,2]. if set > 0, increasing the model's likelihood to talk about new topics.
  response_format: # another type is json
    type: "text"
  stop: ["```"]  # can be set to `String` or `List`. When meeting these words. API would stop to analyze more tokens.
  stream: false
  stream_options: null
  temperature: 2  # [deepseek official API config]: range from [0, 2]. Higher values like 0.8 will make the output more random
  top_p: 1  # An alternative to sampling with temperature, called nucleus sampling. We generally recommend altering this or `temperature` but not both
  tools: null  # A list of tools the model may call.
  tool_choice: "none"
  logprobs: false
  top_logprobs: null