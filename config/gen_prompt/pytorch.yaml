# This file includes the prompt configuration
system_content: |
  You are a {lib} fuzzer, your task is to generate {lib} model and its tensor inputs.
success: |
  ### Shown here is a PyTorch model and its tensor inputs, along with its result analysis after execution on Eager and Inductor.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(nn.Module):
      def __init__(self):
          super(Model, self).__init__()
          self.fc1 = nn.Linear(in_features=28 * 28, out_features=128)
          self.relu = nn.ReLU()
          self.fc2 = nn.Linear(in_features=128, out_features=10)

      def forward(self, x):
          out = self.fc1(x)
          out = self.relu(out)
          out = self.fc2(out)
          return F.log_softmax(out, dim=-1)



  m = Model()

  x = torch.randn(1, 28 * 28)  # Batch size of 1, input size of 28*28 for MNIST
  ```


  ### Result Analysis
  1. **Explanation**: This model triggers new coverage when executed on Eager and Inductor.
  2. **Reasons**: Operator fusion of Linear and ReLu is triggered by the Torch Inductor compiler, leading to new coverage. Additionally, Use a new operator logo_softmax which maybe not appear in the previous test cases.
  3. **Next Testing Strategy**: Consider another optimization of PyTorch Inductor: operator fusion of Conv2d-BatchNorm2d-ReLu.

  ### Suggested APIs to use
  torch.nn.Linear
  torch.nn.Functional.log_softmax


  ### Base on the above PyTorch code and result analysis, generate new PyTorch code to trigger new coverage with suggested PyTorch APIs. Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  The maximum number of operators you can use in forward function is 8.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(nn.Module):
      def __init__(self):
          super(Model, self).__init__()
          self.fc1 = nn.Linear(in_features=28 * 28, out_features=128)
          self.fc2 = nn.Linear(in_features=128 // 4, out_features=10)  # Adjusted for pooling

      def forward(self, x):
          # Flatten the input if necessary
          x = x.view(x.size(0), -1)
          out = self.fc1(x)
          out = F.relu(out)
          # Reshape for max_pool2d
          out = out.view(out.size(0), 1, 16, 8)
          out = F.max_pool2d(out, kernel_size=2, stride=2)
          # Flatten again before fc2
          out = out.view(out.size(0), -1)
          out = self.fc2(out)
          return F.log_softmax(out, dim=-1)


  m = Model()

  x = torch.randn(1, 1, 28, 28)
  ```



  ### Shown here is a PyTorch model and its tensor inputs, along with its result analysis after execution on Eager and Inductor.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(nn.Module):
      def __init__(self):
          super().__init__()
          self.conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
          self.bn = nn.BatchNorm2d(16)
          self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

      def forward(self, x):
          x = self.conv(x)
          x = self.bn(x)
          x = F.relu(x)
          x = self.pool(x)
          return x


  m = Model()

  x = torch.randn(1, 3, 256, 256)
  ```

  ### Result Analysis
  1. **Explanation**: This model does not trigger new coverage when executed on Eager and Inductor.
  2. **Reasons**: The model uses common operators such as Conv2d, BatchNorm2d, ReLU, and MaxPool2d, which are already covered by existing test cases.
  3. **Next Testing Strategy**: The input tensor shape (1, 3, 256, 256) is a common input size for image classification tasks, leading to limited coverage improvement.


  ### Suggested APIs to use
  torch.nn.Conv2d
  torch.nn.GroupNorm
  torch.nn.ConvTranspose2d


  ### Base on the above PyTorch code and result analysis, generate new PyTorch code to trigger new coverage with suggested PyTorch APIs. Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  The maximum number of operators you can use in forward function is 6.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(nn.Module):
      def __init__(self):
          super().__init__()
          self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
          self.gn1 = nn.GroupNorm(num_groups=4, num_channels=16)
          self.conv2 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=2, stride=2, padding=0)
          self.gn2 = nn.GroupNorm(num_groups=2, num_channels=8)
          self.relu = nn.ReLU()

      def forward(self, x):
          x = self.conv1(x)
          x = self.gn1(x)
          x = self.relu(x)
          x = self.conv2(x)
          x = self.gn2(x)
          x = self.relu(x)
          return x

  m = Model()

  x = torch.randn(1, 3, 256, 256)
  ```




  ### Shown here is a PyTorch model and its tensor inputs, along with its result analysis after execution on Eager and Inductor.
  # PyTorch model definition, initialization, and input tensors
  ```python
  {code}
  ```

  ### Result Analysis
  {als_res}

  ### Suggested APIs to use
  {new_ops}

  ### Base on the above PyTorch code and result analysis, generate new PyTorch code to trigger new coverage with suggested PyTorch APIs. Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  The maximum number of operators you can use in forward function is {op_nums}.
  # PyTorch model definition, initialization, and input tensors



failure: |
  ### Shown here is a PyTorch model and its tensor inputs, along with its result analysis after execution on Eager and Inductor.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(torch.nn.Module):
      def __init__(self):
          super(Model, self).__init__()

      def forward(self, x):
          x = x * 2  # Potential type promotion with integer types
          x = torch.nn.functional.dropout(x, p=0.5)  # Dropout may not handle integer types correctly
          x = torch.relu(x)
          return x

  m = Model()

  inputs = torch.tensor([[1, 2, 3], [4, 5, 6]])
  ```


  ### Result Analysis
  1. **Explanation**: This is a potential bug in PyTorch Inductor Compiler.
  2. **Reasons**: The compiler fails to handle type promotion during operations such as dropout, causing a mismatch between input and output tensor types.
  3. **Next Testing Strategy**: Use tensor inputs which involve different dtypes (e.g., int8, int16, int64)


  ### Suggested APIs to use
  torch.nn.functional.dropout
  torch.relu


  ### Base on the above PyTorch code and result analysis, generate new PyTorch code to trigger new coverage or fix invalid models with suggested PyTorch APIs. Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  The maximum number of operators you can use in forward function is 3.
  # PyTorch model definition, initialization, and input tensors
  class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x):
        x = x * 2  # Potential type promotion with integer types
        x = torch.nn.functional.dropout(x, p=0.5)  # Dropout may not handle integer types correctly
        x = torch.relu(x)
        return x

  m = Model()

  inputs_int8 = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int8)



  ### Shown here is a PyTorch model and its tensor inputs, along with its result analysis after execution on Eager and Inductor.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.mm_layer = torch.nn.Linear(10, 10)

      def forward(self, x1, inp):
          v1 = torch.mm(x1, inp)
          v2 = v1 + inp
          return v2

  m = Model()

  x1 = torch.randn(2, 10)
  inp = torch.randn(2, 10)
  ```


  Analyze whether this is an invalid model caused by the code itself or a potential bug in the PyTorch. Help me summarize the symptoms of this bug in three short sentences and give some selected operators for next testing strategy.
  ### Result Analysis
  1. **Explanation**: This is an invalid model caused by the code itself. We should fix it.
  2. **Reasons**: Both x1 and inp have shapes (2, 10), which cannot be multiplied using torch.mm()
  3. **Next Testing Strategy**: Change the shape of the x1 tensor to [2, 2] to match the inp tensor shape.


  ### Suggested APIs to use
  torch.nn.Linear
  torch.mm


  ### Base on the above PyTorch code and result analysis, generate new PyTorch code to trigger new similar bugs or fix invalid models with suggested PyTorch APIs. Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  The maximum number of operators you can use in forward function is 3.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.mm_layer = torch.nn.Linear(10, 10)

      def forward(self, x1, inp):
          v1 = torch.mm(x1, inp)
          v2 = v1 + inp
          return v2

  m = Model()

  x1 = torch.randn(2, 2)
  inp = torch.randn(2, 10)
  ```



  ### Shown here is a PyTorch model and its tensor inputs, along with its result analysis after execution on Eager and Inductor.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(nn.Module):
      def __init__(self):
          super().__init__()
          self.pool = nn.FractionalMaxPool2d(kernel_size=(1, 1), output_ratio=(0.5, 0.5))

      def forward(self, x):
          torch.manual_seed(0)
          x = self.pool(x)
          return x


  model = Model().eval()


  x = torch.randn(1, 1, 10, 10)
  inputs = [x]
  ```


  ### Result Analysis
  1. **Explanation**: This is a potential bug in PyTorch Inductor compiler. The model produces different outputs when executed on eager and compiler.
  2. **Reasons**: FractionalMaxPool2d is incorrectly optimized in the compiler.
  3. **Next Testing Strategy**: Try to use FractionalMaxPool3d operator because it is different from FractionalMaxPool2d operators only in dimension, there may be similar bugs.


  ### Suggested APIs to use
  torch.nn.Sigmoid
  torch.nn.InstanceNorm2d

  ### Base on the above PyTorch code and result analysis, generate new PyTorch code to trigger new similar bugs or fix invalid models with suggested PyTorch APIs. Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  The maximum number of operators you can use in forward function is 5.
  # PyTorch model definition, initialization, and input tensors
  class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.pool = nn.FractionalMaxPool3d(kernel_size=(1, 1, 1), output_ratio=(0.5, 0.5, 0.5))
        self.norm = nn.InstanceNorm2d(1)  # InstanceNorm2d for 2D input
        self.activation = nn.Sigmoid()   # Sigmoid activation

    def forward(self, x):
        torch.manual_seed(0)
        x = self.pool(x)  # Apply FractionalMaxPool3d
        x = x.squeeze(-1)  # Remove the last dimension to make it compatible with InstanceNorm2d
        x = self.norm(x)   # Apply InstanceNorm2d
        x = self.activation(x)
        return x

  model = Model().eval()


  x = torch.randn(1, 1, 10, 10, 10)
  inputs = [x]

  ### Shown here is a PyTorch model and its tensor inputs, along with its result analysis after execution on Eager and Inductor.
  Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  # PyTorch model definition, initialization, and input tensors
  ```python
  {code}
  ```


  ### Result Analysis
  {als_res}


  ### Suggested APIs to use
  {new_ops}


  ### Base on the above PyTorch code and result analysis, generate new PyTorch code to trigger new similar bugs or fix invalid models with suggested PyTorch APIs. Note that you can also use additional PyTorch public operators to satisfy model constraints, in order to avoid the invalid models.
  The maximum number of operators you can use in forward function is {op_nums}.
  # PyTorch model definition, initialization, and input tensors



default: |
  ### Suggested APIs to use
  torch.nn.Linear
  torch.nn.Functional.relu

  ### Please generate a valid PyTorch model with above suggested APIs and some other PyTorch public APIs. Plus, please also generate the input tensor for the newly generated model. The model should be different from the previous one.
  The maximum number of operators you can use in forward function is 3.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.linear = torch.nn.Linear(10, 10)

      def forward(self, x):
          v1 = self.linear(x)
          v2 = v1 - 9.32925237729762
          v3 = F.relu(v2)
          return v3

  m = Model()

  x = torch.randn(1, 10)
  ```


  ### Suggested APIs to use
  torch.mm


  ### Please generate a valid PyTorch model with above suggested APIs and some other PyTorch public APIs. Plus, please also generate the input tensor for the newly generated model. The model should be different from the previous one.
  The maximum number of operators you can use in forward function is 2.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(torch.nn.Module):
      def __init__(self):
          super().__init__()

      def forward(self, x1, x2, inp):
          v1 = torch.mm(x1, x2)
          v2 = v1 + inp
          return v2

  m = Model()

  x1 = torch.randn(3, 5)
  x2 = torch.randn(5, 3)
  inp = torch.randn(3, 3, 3)
  ```


  ### Suggested APIs to use
  torch.nn.Linear
  torch.matmul

  ### Please generate a valid PyTorch model with above suggested APIs and some other PyTorch public APIs. Plus, please also generate the input tensor for the newly generated model. The model should be different from the previous one.
  The maximum number of operators you can use in forward function is 5.
  # PyTorch model definition, initialization, and input tensors
  ```python
  class Model(torch.nn.Module):
      def __init__(self):
          super(Model, self).__init__()
          self.linear = torch.nn.Linear(15, 10)
          self.weight_matrix = torch.nn.Parameter(torch.randn(10, 7))
          self.dropout = torch.nn.Dropout(0.5)

      def forward(self, x):
          x = self.linear(x)
          x = torch.matmul(x, self.weight_matrix)
          x = F.relu(x)
          x = self.dropout(x)
          x = torch.sigmoid(x)
          return x

  x = torch.randn(32, 15)

  model = Model()
  ```


  ### Suggested APIs to use
  {new_ops}

  #### Please generate a valid PyTorch model with above suggested APIs and some other PyTorch public APIs. Plus, please also generate the input tensor for the newly generated model. The model should be different from the previous one.
  The maximum number of operators you can use in forward function is {op_nums}.
  # PyTorch model definition, initialization, and input tensors
